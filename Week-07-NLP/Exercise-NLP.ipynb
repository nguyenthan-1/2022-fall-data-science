{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise with Natural Language Processing\n",
    "\n",
    "For todays exersice we will be doing two things.  The first is to build the same model with the same data that we did in the lecture, the second will be to build a new model with new data. \n",
    "\n",
    "## PART 1: \n",
    "- 20 Newsgroups Corpus\n",
    "\n",
    "\n",
    "## PART 2:\n",
    "- Republican vs Democrat Tweet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nguyenthucthan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nguyenthucthan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nguyenthucthan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import pandas for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK is our Natural-Language-Took-Kit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Libraries for helping us with strings\n",
    "import string\n",
    "# Regular Expression Library\n",
    "import re\n",
    "\n",
    "# Import our text vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Import our classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Import some ML helper function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Import our metrics to evaluate our model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# You may need to download these from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and display data.\n",
    "1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "1. Print the shape\n",
    "1. Inspect / remove nulls and duplicates\n",
    "1. Find class balances, print out how many of each topic_category there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "# 2. Print the shape\n",
    "df = pd.read_csv(\"data/20-newsgroups.csv\")\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Inspect / remove nulls and duplicates\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.sport.hockey            600\n",
       "soc.religion.christian      599\n",
       "rec.motorcycles             598\n",
       "rec.sport.baseball          597\n",
       "sci.crypt                   595\n",
       "sci.med                     594\n",
       "rec.autos                   594\n",
       "comp.windows.x              593\n",
       "sci.space                   593\n",
       "comp.os.ms-windows.misc     591\n",
       "sci.electronics             591\n",
       "comp.sys.ibm.pc.hardware    590\n",
       "misc.forsale                585\n",
       "comp.graphics               584\n",
       "comp.sys.mac.hardware       578\n",
       "talk.politics.mideast       564\n",
       "talk.politics.guns          546\n",
       "alt.atheism                 480\n",
       "talk.politics.misc          465\n",
       "talk.religion.misc          377\n",
       "Name: topic_category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Find class balances, print out how many of each topic_category there are.\n",
    "df.topic_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing \n",
    "(aka Feature engineering)\n",
    "1. Make a function that makes all text lowercase.\n",
    "    * Do a sanity check by feeding in a test sentence into the function. \n",
    "    \n",
    "    \n",
    "2. Make a function that removes all punctuation. \n",
    "    * Do a sanity check by feeding in a test sentence into the function. \n",
    "    \n",
    "    \n",
    "3. Make a function that removes all stopwords.\n",
    "    * Do a sanity check by feeding in a test sentence into the function. \n",
    "    \n",
    "    \n",
    "4. EXTRA CREDIT (This step only): Make a function that stemms all words. \n",
    "\n",
    "\n",
    "5. Mandatory: Make a pipeline function that applys all the text processing functions you just built.\n",
    "    * Do a sanity check by feeding in a test sentence into the pipeline. \n",
    "    \n",
    "    \n",
    "    \n",
    "6. Mandatory: Use `df['message_clean'] = df[column].apply(???)` and apply the text pipeline to your text data column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sentence with lots of caps.\n"
     ]
    }
   ],
   "source": [
    "# 1. Make a function that makes all text lowercase.\n",
    "def make_lower(a_string):\n",
    "    return a_string.lower()\n",
    "test_string = 'This is A SENTENCE with LOTS OF CAPS.'\n",
    "test_string = make_lower(test_string)\n",
    "print(test_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sentence 50 With lots of punctuation  other things'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Make a function that removes all punctuation. \n",
    "def remove_punctuation(s):\n",
    "    return re.sub(r'[^\\w\\s]', '', s)\n",
    "\n",
    "test_string = 'This is a sentence! 50 With lots of punctuation??? & other #things.'\n",
    "remove_punctuation(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This sentence ! With different stopwords added .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Make a function that removes all stopwords.\n",
    "def remove_stopwords(s):\n",
    "    words = word_tokenize(s)\n",
    "    valid_words = [word for word in words if word not in stopwords ]\n",
    "    return \" \".join(valid_words)\n",
    "\n",
    "test_string = 'This is a sentence! With some different stopwords i have added in here.'\n",
    "remove_stopwords(test_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. EXTRA CREDIT: Make a function that stemms all words. \n",
    "\n",
    "test_string = 'I played and started playing with players and we all love to play with plays'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MANDATORY: Make a pipeline function that applys all the text processing functions you just built.\n",
    "\n",
    "\n",
    "test_string = 'I played and started playing with players and we all love to play with plays'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Mandatory: Use `df[column].apply(???)` and apply the text pipeline to your text data column. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "\n",
    "1. Define your `X` and `y` data. \n",
    "\n",
    "\n",
    "2. Initialize a vectorizer (you can use TFIDF or BOW, it is your choice).\n",
    "    * Do you want to use n-grams..?\n",
    "\n",
    "\n",
    "3. Fit your vectorizer using your X data.\n",
    "    * Remember, this process happens IN PLACE.\n",
    "\n",
    "\n",
    "4. Transform your X data using your fitted vectorizer. \n",
    "    * `X = vectorizer.???`\n",
    "\n",
    "\n",
    "\n",
    "5. Print the shape of your X.  How many features (aka columns) do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your `X` and `y` data. \n",
    "# 1. Define your `X` and `y` data. \n",
    "X=df['message'].values\n",
    "y=df['topic_category'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize a vectorizer (you can use TFIDF or BOW, it is your choice).\n",
    "vectorizer=TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Fit your vectorizer using your X data\n",
    "\n",
    "vectorizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transform your X data using your fitted vectorizer. \n",
    "\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "# 5. Print the shape of your X.  How many features (aka columns) do you have?\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split your data into Training and Testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into testing and training like always. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Build and Train Model\n",
    "Use Multinomial Naive Bayes to classify these documents. \n",
    "\n",
    "1. Initalize an empty model. \n",
    "2. Fit the model with our training data.\n",
    "\n",
    "\n",
    "Experiment with different alphas.  Use the alpha gives you the best result.\n",
    "\n",
    "EXTRA CREDIT:  Use grid search to programmatically do this for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initalize an empty model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model with our training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model.\n",
    "\n",
    "1. Make new predicitions using our test data. \n",
    "2. Print the accuracy of the model. \n",
    "3. Print the confusion matrix of our predictions. \n",
    "4. Using `classification_report` print the evaluation results for all the classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make new predictions of our testing data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Print the accuracy of the model. \n",
    "accuracy = ???\n",
    "\n",
    "print(\"Model Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Plot the confusion matrix of our predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Using `classification_report` print the evaluation results for all the classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual predicition\n",
    "Write a new sentence that you think will be classified as talk.politics.guns. \n",
    "1. Apply the text pipeline to your sentence\n",
    "2. Transform your cleaned text using the `X = vectorizer.transform([your_text])`\n",
    "    * Note, the `transform` function accepts a list and not a individual string.\n",
    "3. Use the model to predict your new `X`. \n",
    "4. Print the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = ???\n",
    "\n",
    "# 1. Apply the text pipeline to your sentence\n",
    "\n",
    "# 2. Transform your cleaned text using the `X = vectorizer.transform([your_text])`\\\n",
    "\n",
    "# 3. Use the model to predict your new `X`. \n",
    "\n",
    "# 4. Print the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# PART 2: Twitter Data\n",
    "This part of the exercise is un-guided on purpose.  \n",
    "\n",
    "Using the `dem-vs-rep-tweets.csv` build a classifier to determine if a tweet was written by a democrat or republican. \n",
    "\n",
    "Can you get an f1-score higher than %82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the 20-newsgroups.csv data into a dataframe.\n",
    "# 2. Print the shape\n",
    "df = pd.read_csv('data/dem-vs-rep-tweets.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
